{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom environment must define these core methods:\n",
    "\n",
    "* __init__: Initialize the environment.\n",
    "* reset: Reset the environment to its initial state and return the initial observation.\n",
    "* step: Take an action and return (observation, reward, done, truncated, info).\n",
    "* render (optional): Visualize the environment (if needed).\n",
    "* close (optional): Clean up resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is environment that enable the drone to follow the balloon. In this environment, the ball didn't respawn randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class DroneChaseEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        super(DroneChaseEnv, self).__init__()\n",
    "        \n",
    "        # Canvas size\n",
    "        self.width = 600\n",
    "        self.height = 400\n",
    "        \n",
    "        # Score\n",
    "        self.score = 0  # Tracks how many times the drone hits the balloon\n",
    "\n",
    "        # Drone parameters\n",
    "        self.drone_pos = np.array([self.width / 2, self.height / 2], dtype=np.float32)\n",
    "        self.drone_vel = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        self.max_speed = 5.0\n",
    "\n",
    "        # Balloon parameters\n",
    "        self.balloon_pos = np.array([np.random.uniform(0, self.width), \n",
    "                                     np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        self.balloon_vel = np.array([np.random.uniform(-1, 1), \n",
    "                                     np.random.uniform(-1, 1)], dtype=np.float32)\n",
    "\n",
    "        # Action space: Continuous thrust adjustments for left and right propellers\n",
    "        # Thrust values between -1 and 1\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        # Observation space: Positions and velocities\n",
    "        # Drone position (x, y), Drone velocity (vx, vy), Balloon position (x, y)\n",
    "        low_obs = np.array([0, 0, -self.max_speed, -self.max_speed, 0, 0], dtype=np.float32)\n",
    "        high_obs = np.array([self.width, self.height, self.max_speed, self.max_speed, self.width, self.height], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low_obs, high=high_obs, dtype=np.float32)\n",
    "\n",
    "        # Rendering\n",
    "        self.render_mode = render_mode\n",
    "        if self.render_mode == 'human':\n",
    "            import pygame\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        # Episode parameters\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset drone position and velocity\n",
    "        self.drone_pos = np.array([self.width / 2, self.height / 2], dtype=np.float32)\n",
    "        self.drone_vel = np.array([0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "        # Reset balloon position and velocity\n",
    "        self.balloon_pos = np.array([np.random.uniform(0, self.width), \n",
    "                                     np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        self.balloon_vel = np.array([np.random.uniform(-1, 1), \n",
    "                                     np.random.uniform(-1, 1)], dtype=np.float32)\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Update drone velocity based on action (thrust adjustments)\n",
    "        left_thrust, right_thrust = action\n",
    "        thrust = np.array([right_thrust - left_thrust, (left_thrust + right_thrust) / 2], dtype=np.float32)\n",
    "        \n",
    "        # Update drone velocity and position\n",
    "        self.drone_vel += thrust\n",
    "        \n",
    "        # Limit speed\n",
    "        speed = np.linalg.norm(self.drone_vel)\n",
    "        if speed > self.max_speed:\n",
    "            self.drone_vel = (self.drone_vel / speed) * self.max_speed\n",
    "        self.drone_pos += self.drone_vel\n",
    "\n",
    "        # Update balloon position\n",
    "        self.balloon_pos += self.balloon_vel\n",
    "        \n",
    "        # Keep balloon within bounds\n",
    "        self.balloon_pos = np.clip(self.balloon_pos, [0, 0], [self.width, self.height])\n",
    "\n",
    "        # Keep drone within bounds\n",
    "        self.drone_pos = np.clip(self.drone_pos, [0, 0], [self.width, self.height])\n",
    "\n",
    "        # Compute distance to balloon\n",
    "        distance = np.linalg.norm(self.drone_pos - self.balloon_pos)\n",
    "\n",
    "        # Reward: Negative distance to balloon\n",
    "        reward = -distance * 0.01  # Scale down the reward\n",
    "\n",
    "        # Check if the drone catches the balloon (within a certain radius)\n",
    "        done = False\n",
    "        catch_radius = 10.0\n",
    "        if distance < catch_radius:\n",
    "            reward += 10.0  # Bonus for catching the balloon\n",
    "            self.score += 1  # Increment the score\n",
    "            done = True\n",
    "\n",
    "        # Penalty for each time step\n",
    "        reward -= 0.1\n",
    "\n",
    "        # Increment step count\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "\n",
    "        return observation, reward, done, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != 'human':\n",
    "            return\n",
    "\n",
    "        import pygame\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "        self.screen.fill((255, 255, 255))  # White background\n",
    "\n",
    "        # Draw balloon\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), self.balloon_pos.astype(int), 10)  # Red circle\n",
    "\n",
    "        # Draw drone\n",
    "        pygame.draw.rect(self.screen, (0, 0, 255), (*self.drone_pos - 10, 20, 20))  # Blue square\n",
    "\n",
    "        # Add labels\n",
    "        font = pygame.font.Font(None, 24)  # Default font, size 24\n",
    "        balloon_label = font.render(\"Balloon\", True, (0, 0, 0))  # Black text for balloon\n",
    "        drone_label = font.render(\"Drone\", True, (0, 0, 0))  # Black text for drone\n",
    "\n",
    "        # Position labels near objects\n",
    "        self.screen.blit(balloon_label, (self.balloon_pos[0] - 20, self.balloon_pos[1] - 20))\n",
    "        self.screen.blit(drone_label, (self.drone_pos[0] - 20, self.drone_pos[1] - 20))\n",
    "        \n",
    "        # Display timer\n",
    "        time_remaining = max(0, 30 - self.current_step / 30)  # 30 FPS assumed\n",
    "        timer_label = font.render(f\"Time Remaining: {time_remaining:.1f}s\", True, (0, 0, 0))\n",
    "        self.screen.blit(timer_label, (10, 10))\n",
    "        \n",
    "        # Display the score\n",
    "        score_label = font.render(f\"Score: {self.score}\", True, (0, 0, 0))\n",
    "        self.screen.blit(score_label, (10, 40))  # Display below the timer\n",
    "        \n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(30)\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode == 'human':\n",
    "            import pygame\n",
    "            pygame.quit()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate((self.drone_pos, self.drone_vel, self.balloon_pos)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is environment that enable the drone to follow the balloon. In this environment, the ball will respawn randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "\n",
    "class DroneChaseEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        super(DroneChaseEnv, self).__init__()\n",
    "        \n",
    "        # Canvas size\n",
    "        self.width = 600\n",
    "        self.height = 400\n",
    "        \n",
    "        # Score\n",
    "        self.score = 0  # Tracks how many times the drone hits the balloon\n",
    "\n",
    "        # Drone parameters\n",
    "        self.drone_pos = np.array([self.width / 2, self.height / 2], dtype=np.float32)\n",
    "        self.drone_vel = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        self.max_speed = 5.0\n",
    "\n",
    "        # Balloon parameters\n",
    "        self.balloon_pos = np.array([np.random.uniform(0, self.width), \n",
    "                                     np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        self.balloon_vel = np.array([np.random.uniform(-1, 1), \n",
    "                                     np.random.uniform(-1, 1)], dtype=np.float32)\n",
    "\n",
    "        # Action space: Continuous thrust adjustments for left and right propellers\n",
    "        # Thrust values between -1 and 1\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        # Observation space: Positions and velocities\n",
    "        # Drone position (x, y), Drone velocity (vx, vy), Balloon position (x, y)\n",
    "        low_obs = np.array([0, 0, -self.max_speed, -self.max_speed, 0, 0], dtype=np.float32)\n",
    "        high_obs = np.array([self.width, self.height, self.max_speed, self.max_speed, self.width, self.height], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low_obs, high=high_obs, dtype=np.float32)\n",
    "\n",
    "        # Rendering\n",
    "        self.render_mode = render_mode\n",
    "        if self.render_mode == 'human':\n",
    "            import pygame\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        # Episode parameters\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset drone position and velocity\n",
    "        self.drone_pos = np.array([self.width / 2, self.height / 2], dtype=np.float32)\n",
    "        self.drone_vel = np.array([0.0, 0.0], dtype=np.float32)\n",
    "\n",
    "        # Reset balloon position and velocity\n",
    "        self.balloon_pos = np.array([np.random.uniform(0, self.width), \n",
    "                                     np.random.uniform(0, self.height)], dtype=np.float32)\n",
    "        self.balloon_vel = np.array([np.random.uniform(-1, 1), \n",
    "                                     np.random.uniform(-1, 1)], dtype=np.float32)\n",
    "\n",
    "        self.current_step = 0\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Executes one time step in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (array): Array of two thrust values [left_thrust, right_thrust], \n",
    "                            where each value is between -1 and 1.\n",
    "\n",
    "        Returns:\n",
    "            observation (array): Updated state containing [drone_position, drone_velocity, balloon_position].\n",
    "            reward (float): The reward for the current step.\n",
    "            done (bool): Whether the episode has ended (always False for continuous simulation).\n",
    "            truncated (bool): Whether the episode was truncated (always False here).\n",
    "            info (dict): Additional debugging information (empty here).\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Calculate the drone's thrust based on the action\n",
    "        # The left and right thrust values determine the horizontal and vertical motion of the drone.\n",
    "        left_thrust, right_thrust = action\n",
    "        thrust = np.array([right_thrust - left_thrust,  # Horizontal movement (right thrust - left thrust)\n",
    "                        (left_thrust + right_thrust) / 2],  # Vertical movement (average thrust)\n",
    "                        dtype=np.float32)\n",
    "\n",
    "        # 2. Update the drone's velocity based on the thrust\n",
    "        self.drone_vel += thrust\n",
    "\n",
    "        # 3. Enforce the maximum speed limit\n",
    "        # If the drone's speed exceeds the allowed max_speed, scale it down to max_speed.\n",
    "        speed = np.linalg.norm(self.drone_vel)  # Calculate the magnitude of the velocity vector\n",
    "        if speed > self.max_speed:\n",
    "            self.drone_vel = (self.drone_vel / speed) * self.max_speed  # Scale the velocity vector\n",
    "\n",
    "        # 4. Update the drone's position based on the velocity\n",
    "        self.drone_pos += self.drone_vel\n",
    "\n",
    "        # 5. Ensure the drone stays within the canvas bounds\n",
    "        # If the position goes outside the defined environment, clip it to the boundaries.\n",
    "        self.drone_pos = np.clip(self.drone_pos, [0, 0], [self.width, self.height])\n",
    "\n",
    "        # 6. Calculate the distance between the drone and the balloon\n",
    "        # Use the Euclidean distance formula to determine how far the drone is from the balloon.\n",
    "        distance = np.linalg.norm(self.drone_pos - self.balloon_pos)\n",
    "\n",
    "        # 7. Check if the drone catches the balloon (within the catch radius)\n",
    "        catch_radius = 10.0  # Defines the radius within which the drone \"catches\" the balloon\n",
    "        if distance < catch_radius:\n",
    "            # If the drone catches the balloon:\n",
    "            # - Increment the score\n",
    "            self.score += 1\n",
    "            # - Respawn the balloon at a random location\n",
    "            self.balloon_pos = np.random.uniform([0, 0], [self.width, self.height])\n",
    "            # - Recalculate the distance to the new balloon position\n",
    "            distance = np.linalg.norm(self.drone_pos - self.balloon_pos)\n",
    "            # - Give a large reward for catching the balloon\n",
    "            reward = 10.0\n",
    "        else:\n",
    "            # If the balloon is not caught, penalize the drone based on the distance\n",
    "            # Closer to the balloon = less penalty; farther away = more penalty\n",
    "            reward = -distance * 0.01\n",
    "\n",
    "        # 8. Randomly move the balloon\n",
    "        # The balloon moves in small, random steps within the environment bounds.\n",
    "        self.balloon_pos += np.random.uniform(-2, 2, size=(2,))\n",
    "        self.balloon_pos = np.clip(self.balloon_pos, [0, 0], [self.width, self.height])  # Keep balloon in bounds\n",
    "\n",
    "        # 9. Penalize the drone for each time step to encourage efficiency\n",
    "        reward -= 0.1\n",
    "\n",
    "        # 10. Increment the step count\n",
    "        # Track how many steps have been taken in the episode.\n",
    "        self.current_step += 1\n",
    "\n",
    "        # 11. Create the observation array\n",
    "        # The observation includes the drone's position, velocity, and the balloon's position.\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        # 12. Return the updated state, reward, and additional info\n",
    "        # 'done' and 'truncated' are False here because this is a continuous simulation.\n",
    "        info = {}\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "        return observation, reward, False, False, info\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != 'human':\n",
    "            return\n",
    "\n",
    "        import pygame\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "        self.screen.fill((255, 255, 255))  # White background\n",
    "\n",
    "        # Draw balloon\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), self.balloon_pos.astype(int), 10)  # Red circle\n",
    "\n",
    "        # Draw drone\n",
    "        pygame.draw.rect(self.screen, (0, 0, 255), (*self.drone_pos - 10, 20, 20))  # Blue square\n",
    "\n",
    "        # Add labels\n",
    "        font = pygame.font.Font(None, 24)  # Default font, size 24\n",
    "        balloon_label = font.render(\"Balloon\", True, (0, 0, 0))  # Black text for balloon\n",
    "        drone_label = font.render(\"Drone\", True, (0, 0, 0))  # Black text for drone\n",
    "\n",
    "        # Position labels near objects\n",
    "        self.screen.blit(balloon_label, (self.balloon_pos[0] - 20, self.balloon_pos[1] - 20))\n",
    "        self.screen.blit(drone_label, (self.drone_pos[0] - 20, self.drone_pos[1] - 20))\n",
    "        \n",
    "        # Display timer\n",
    "        time_remaining = max(0, 30 - self.current_step / 30)  # 30 FPS assumed\n",
    "        timer_label = font.render(f\"Time Remaining: {time_remaining:.1f}s\", True, (0, 0, 0))\n",
    "        self.screen.blit(timer_label, (10, 10))\n",
    "        \n",
    "        # Display the score\n",
    "        score_label = font.render(f\"Score: {self.score}\", True, (0, 0, 0))\n",
    "        self.screen.blit(score_label, (10, 40))  # Display below the timer\n",
    "        \n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(30)\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode == 'human':\n",
    "            import pygame\n",
    "            pygame.quit()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.concatenate((self.drone_pos, self.drone_vel, self.balloon_pos)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To train the PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create vectorized environment for parallel training\n",
    "env = make_vec_env(lambda: DroneChaseEnv(), n_envs=4)\n",
    "\n",
    "# Define the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=800000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"ppo_drone_chase\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: -1308.3312060093917\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(\"ppo_drone_chase\")\n",
    "\n",
    "# Create the environment with rendering enabled\n",
    "env = DroneChaseEnv(render_mode='human')\n",
    "\n",
    "# Reset the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "#num_steps = 300  # Simulate for 300 steps (10 seconds at 30 FPS)\n",
    "num_steps = 30 * 30  # 30 seconds * 30 FPS = 900 steps\n",
    "\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "\n",
    "#while not done:\n",
    "for _ in range(num_steps):\n",
    "\n",
    "    # Get action from the model\n",
    "    action, _ = model.predict(observation, deterministic=True)\n",
    "    \n",
    "    # Step the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test and evaluate the model and show the plot result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(observation, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m observation, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Log reward and score\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 151\u001b[0m, in \u001b[0;36mDroneChaseEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    149\u001b[0m info \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, info\n",
      "Cell \u001b[1;32mIn[2], line 162\u001b[0m, in \u001b[0;36mDroneChaseEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m pygame\u001b[38;5;241m.\u001b[39mQUIT:\n\u001b[0;32m    161\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mquit()\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# White background\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m# Draw balloon\u001b[39;00m\n\u001b[0;32m    165\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mcircle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mballoon_pos\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), \u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Red circle\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained model\n",
    "model = PPO.load(\"ppo_drone_chase\")\n",
    "\n",
    "# Parameters for evaluation\n",
    "#num_steps = 300  # Simulate for 300 steps (10 seconds at 30 FPS)\n",
    "num_steps = 30 * 30  # 30 seconds * 30 FPS = 900 steps\n",
    "\n",
    "rewards = []  # Store rewards for each step\n",
    "scores = []  # Store scores (number of catches)\n",
    "steps = list(range(num_steps))  # Step indices\n",
    "\n",
    "# Create the environment with rendering enabled\n",
    "env = DroneChaseEnv(render_mode='human')\n",
    "\n",
    "# Reset the environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "done = False\n",
    "total_reward = 0.0\n",
    "\n",
    "#while not done:\n",
    "for _ in range(num_steps):\n",
    "\n",
    "    # Get action from the model\n",
    "    action, _ = model.predict(observation, deterministic=True)\n",
    "    \n",
    "    # Step the environment\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Log reward and score\n",
    "    rewards.append(reward)\n",
    "    scores.append(env.score)\n",
    "    \n",
    "# Save results to CSV\n",
    "csv_file = \"reward_performance.csv\"\n",
    "with open(csv_file, mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Step\", \"Reward\", \"Score\"])\n",
    "    for step, reward, score in zip(steps, rewards, scores):\n",
    "        writer.writerow([step, reward, score])\n",
    "\n",
    "# Plot reward performance\n",
    "plt.plot(steps, rewards, label=\"Reward\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.title(\"Reward Performance Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot cumulative score performance\n",
    "plt.plot(steps, scores, label=\"Score\", color=\"orange\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Cumulative Score Over Time\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(f\"Total Reward: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
